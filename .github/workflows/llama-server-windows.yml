# Server Windows build with CPU optimizations
name: Llama Server Windows

on:
  workflow_dispatch:
    inputs:
      create_release:
        description: 'Create a release'
        required: false
        default: false
        type: boolean

# Add explicit permissions for creating releases
permissions:
  contents: write
  packages: read

env:
  LLAMA_LOG_COLORS: 1
  LLAMA_LOG_PREFIX: 1
  LLAMA_LOG_TIMESTAMPS: 1
  LLAMA_LOG_VERBOSITY: 10

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}-${{ github.head_ref || github.run_id }}
  cancel-in-progress: true

jobs:
  windows:
    runs-on: windows-2019
    strategy:
      fail-fast: false
      matrix:
        include:
          - build: 'generic'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF'
          - build: 'sse42'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_SSE4_2=ON'
          - build: 'sandybridge'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=OFF'
          - build: 'haswell'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON'
          - build: 'skylakex'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON'
          - build: 'icelake'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON'
          - build: 'alderlake'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX_VNNI=ON'
          - build: 'sapphirerapids'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON'
          - build: 'zen4'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON'

    steps:
      - name: Clone
        id: checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Build
        id: cmake_build
        run: |
          cmake -B build -DLLAMA_BUILD_SERVER=ON -DLLAMA_FATAL_WARNINGS=OFF -DLLAMA_CURL=OFF -DBUILD_SHARED_LIBS=OFF -DCMAKE_BUILD_TYPE=Release ${{ matrix.defines }}
          cmake --build build --config Release -j ${env:NUMBER_OF_PROCESSORS} --target llama-server
          
      # - name: Python setup
      #   id: setup_python
      #   uses: actions/setup-python@v5
      #   with:
      #     python-version: '3.11'

      # - name: Tests dependencies
      #   id: test_dependencies
      #   run: |
      #     pip install -r examples/server/tests/requirements.txt

      # - name: Set environment variables
      #   shell: pwsh
      #   run: |
      #     echo "USER_HOME=${HOME}" >> $env:GITHUB_ENV

      # - name: Cache HuggingFace models
      #   uses: actions/cache@v4
      #   id: cache-hf
      #   with:
      #     path: ${{ env.USER_HOME }}\.cache\huggingface
      #     key: hf-cache-Windows-llama2-7b-chat-${{ matrix.build }}

      # - name: Check and Download Llama model
      #   if: steps.cache-hf.outputs.cache-hit != 'true'
      #   run: |
      #     python -m pip install -U pip
      #     python -m pip install -U "huggingface_hub[cli]"
      #     huggingface-cli download --revision 191239b3e26b2882fb562ffccdd1cf0f65402adb TheBloke/Llama-2-7B-Chat-GGUF llama-2-7b-chat.Q4_K_M.gguf

      # - name: Tests
      #   id: server_integration_tests
      #   run: |
      #     cd examples/server/tests
      #     $env:PYTHONIOENCODING = ":replace"
      #     pytest -v -x

      # - name: Slow tests
      #   id: server_integration_tests_slow
      #   if: ${{ github.event.schedule || github.event.inputs.slow_tests == 'true' }}
      #   run: |
      #     cd examples/server/tests
      #     $env:SLOW_TESTS = "1"
      #     pytest -v -x 
      
      # Keep the artifact upload for workflow runs
      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-windows-${{ matrix.build }}
          path: build/bin/Release/llama-server.exe
          compression-level: 0

  macos:
    runs-on: macos-14
    strategy:
      fail-fast: false
      matrix:
        variant: [metal, cpu]
        include:
          - variant: metal
            defines: >-
              -DGGML_METAL=ON
              -DGGML_METAL_EMBED_LIBRARY=ON
          - variant: cpu
            defines: >-
              -DGGML_METAL=OFF

    steps:
      - name: Clone
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Dependencies
        continue-on-error: true
        run: |
          brew update

      - name: Build
        run: |
          cmake -B build \
            -DCMAKE_BUILD_TYPE=Release \
            -DLLAMA_FATAL_WARNINGS=OFF \
            -DLLAMA_CURL=OFF \
            -DBUILD_SHARED_LIBS=OFF \
            ${{ matrix.defines }}
          cmake --build build --target llama-server -j $(sysctl -n hw.logicalcpu)

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-macos-${{ matrix.variant }}
          path: build/bin/llama-server
          compression-level: 0

  linux:
    runs-on: ubuntu-20.04
    strategy:
      fail-fast: false
      matrix:
        include:
          - build: 'generic'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF'
          - build: 'sse42'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=OFF -DGGML_AVX2=OFF -DGGML_SSE4_2=ON'
          - build: 'sandybridge'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=OFF'
          - build: 'haswell'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON'
          - build: 'skylakex'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON'
          - build: 'icelake'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON'
          - build: 'alderlake'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX_VNNI=ON'
          - build: 'sapphirerapids'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON -DGGML_AVX512_VBMI=ON -DGGML_AVX512_VNNI=ON'
          - build: 'zen4'
            defines: '-DGGML_NATIVE=OFF -DGGML_AVX=ON -DGGML_AVX2=ON -DGGML_AVX512=ON'

    steps:
      - name: Clone
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Build
        run: |
          cmake -B build \
            -DLLAMA_BUILD_SERVER=ON \
            -DLLAMA_FATAL_WARNINGS=OFF \
            -DLLAMA_CURL=OFF \
            -DBUILD_SHARED_LIBS=OFF \
            -DCMAKE_BUILD_TYPE=Release \
            ${{ matrix.defines }}
          cmake --build build --target llama-server -j $(nproc)

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: llama-server-linux-${{ matrix.build }}
          path: build/bin/llama-server
          compression-level: 0

  release:
    runs-on: ubuntu-latest
    needs: [windows, linux, macos]
    if: always() && github.event.inputs.create_release == 'true'
    
    steps:
      - name: Clone
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
      
      # Get the short SHA for tagging the release
      - name: Get short SHA
        id: get_short_sha
        run: echo "sha=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
      
      # Create a directory for artifacts
      - name: Create artifacts directory
        run: mkdir -p artifacts
      
      # Download all artifacts from previous jobs
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: artifacts
      
      # List contents for debugging
      - name: List artifacts
        run: find artifacts -type f | sort
        
      # Check if any artifacts were found
      - name: Check for artifacts
        id: check_artifacts
        run: |
          ARTIFACT_COUNT=$(find artifacts -type f | wc -l)
          echo "artifact_count=$ARTIFACT_COUNT" >> $GITHUB_OUTPUT
          if [ "$ARTIFACT_COUNT" -eq 0 ]; then
            echo "No artifacts found! All builds must have failed."
            exit 1
          else
            echo "Found $ARTIFACT_COUNT artifacts, proceeding with release creation."
          fi
      
      # Rename artifacts for release
      - name: Rename artifacts for release
        run: |
          mkdir -p renamed_artifacts
          for dir in artifacts/*; do
            build_type=$(basename "$dir")
            if [[ -f "$dir/llama-server.exe" ]]; then
              cp "$dir/llama-server.exe" "renamed_artifacts/${build_type}.exe"
              echo "Renamed $dir/llama-server.exe to renamed_artifacts/${build_type}.exe"
            elif [[ -f "$dir/llama-server" ]]; then
              cp "$dir/llama-server" "renamed_artifacts/${build_type}"
              echo "Renamed $dir/llama-server to renamed_artifacts/${build_type}"
            fi
          done
      
      # Create a release using GitHub CLI instead of third-party action
      - name: Create GitHub Release
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          TAG_NAME: build-${{ steps.get_short_sha.outputs.sha }}
          RELEASE_NAME: "Llama Server Build ${{ steps.get_short_sha.outputs.sha }}"
        run: |
          # Create release notes file
          echo "# Llama Server Windows Builds" > release_notes.md
          echo "" >> release_notes.md
          echo "Automated build from commit ${{ steps.get_short_sha.outputs.sha }}" >> release_notes.md
          echo "" >> release_notes.md
          echo "## Available Builds" >> release_notes.md
          
          # List all renamed artifacts in release notes
          find renamed_artifacts -type f | sort | while read file; do
            echo "- $(basename "$file")" >> release_notes.md
          done
          
          # Create the release
          gh release create "$TAG_NAME" \
            --title "$RELEASE_NAME" \
            --notes-file release_notes.md \
            --latest
          
          # Upload all renamed artifacts to the release
          find renamed_artifacts -type f | sort | while read file; do
            gh release upload "$TAG_NAME" "$file" --clobber
          done
